{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())\n",
    "df[\"Outcome\"].value_counts()*100/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True)\n",
    "plt.title('Distribution of Outcome ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Age: \" + str(df[\"Age\"].max()))\n",
    "print(\"Min Age: \" + str(df[\"Age\"].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Values for Features\n",
    "\n",
    "- **Glucose**: 70-99 mg/dL (fasting)  \n",
    "- **Blood Pressure**: <120/80 mmHg  \n",
    "- **Skin Thickness**: 10-50 mm  \n",
    "- **Insulin**: 2.6-24.9 Î¼U/mL (fasting)  \n",
    "- **BMI**: 18.5-24.9  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(15,9))\n",
    "columns = df.columns.drop('Outcome')\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    plt.subplot(3,3, i+1)\n",
    "    sns.boxplot(x=col, data=df)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot( hue= 'Outcome', data= df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(16, 16))\n",
    "\n",
    "sns.histplot(df.Age, bins=20, kde=True, ax=ax[0, 0]) \n",
    "sns.histplot(df.Pregnancies, bins=20, kde=True, ax=ax[0, 1]) \n",
    "sns.histplot(df.Glucose, bins=20, kde=True, ax=ax[1, 0]) \n",
    "sns.histplot(df.BloodPressure, bins=20, kde=True, ax=ax[1, 1]) \n",
    "sns.histplot(df.SkinThickness, bins=20, kde=True, ax=ax[2, 0])\n",
    "sns.histplot(df.Insulin, bins=20, kde=True, ax=ax[2, 1])\n",
    "sns.histplot(df.DiabetesPedigreeFunction, bins=20, kde=True, ax=ax[3, 0]) \n",
    "sns.histplot(df.BMI, bins=20, kde=True, ax=ax[3, 1]) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_means = df.groupby(\"Outcome\").mean()\n",
    "\n",
    "ax = grouped_means.plot(kind=\"bar\", figsize=(15, 6), colormap=\"viridis\", edgecolor=\"black\")\n",
    "plt.title(\"Mean Values of Features by Outcome\", fontsize=16)\n",
    "plt.xlabel(\"Outcome\", fontsize=14)\n",
    "plt.ylabel(\"Mean Values\", fontsize=14)\n",
    "plt.legend(title=\"Features\", fontsize=12)\n",
    "\n",
    "#Add numbers on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", fontsize=10, label_type=\"edge\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(8,6))\n",
    "sns.heatmap(df.corr(),vmin= -1 ,center= 0,cmap='RdBu_r' ,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_replace = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age']\n",
    "df[columns_to_replace] = df[columns_to_replace].replace(0, np.nan)\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_median(column):\n",
    "    median_values = df.groupby('Outcome')[column].median()\n",
    "    \n",
    "    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_values[0]\n",
    "    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_values[1]\n",
    "\n",
    "for col in df.columns.drop('Outcome'):\n",
    "    fill_missing_with_median(col)\n",
    "\n",
    "\n",
    "df.to_csv('decisionTree.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Outcome']\n",
    "cols = ['Glucose', 'Pregnancies','DiabetesPedigreeFunction', 'Insulin', 'BMI', 'Age']\n",
    "X = df[cols]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=41)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#std_df = scaler.fit_transform(X)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)\n",
    "\n",
    "#X_train = pd.DataFrame(X_train, columns=cols, index=X_train.index if hasattr(X_train, 'index') else None)\n",
    "#X_test = pd.DataFrame(X_test, columns=cols, index=X_test.index if hasattr(X_test, 'index') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    # Sigmoid activation function\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # Fit the model using gradient descent\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient descent loop\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Linear combination\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    # Predict probabilities for the test set\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(linear_model)\n",
    "\n",
    "    # Predict binary labels using a threshold\n",
    "    def predict(self, X, threshold=0.6):\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        return (y_pred_proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn import datasets\n",
    "class KNN:\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def value(self,knn_values):\n",
    "        most_common = Counter(knn_values).most_common()\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        #X_train\n",
    "        self.X = X\n",
    "        #y_train\n",
    "        self.Y = Y\n",
    "        \n",
    "    def euclidean_distance(self, point_a, point_b):\n",
    "        return np.linalg.norm(point_a-point_b)\n",
    "    \n",
    "\n",
    "    def predict(self,X):\n",
    "        #assign each distance to a label and then pick the most popular\n",
    "        predictions = []\n",
    "        for test_point in X:\n",
    "            distances = []\n",
    "            train_data = zip(self.X, self.Y)\n",
    "            for train_point, train_label in train_data:\n",
    "                distance = self.euclidean_distance(test_point, train_point)\n",
    "                distances.append((distance, train_label))\n",
    "            distances.sort()\n",
    "            dists = distances[:self.k]\n",
    "            y = [y_ for _, y_ in dists]\n",
    "            predictions.append(self.value(y))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knearest =  KNN()\n",
    "knearest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knearest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on trained model with standardized data\n",
    "# patient is not diabetic 1,103,30,38,83,43.3,0.183,33,0\n",
    "\n",
    "#patient = np.array([1,103])\n",
    "#patient = patient.reshape(1,-1)\n",
    "#print(patient)\n",
    "#y_pred = knearest.predict(patient)\n",
    "\n",
    "\n",
    "#if y_pred[0] == 1:\n",
    " #   print(\"Patient is diabetic\")\n",
    "#else:\n",
    " #   print(\"Patient is not diabetic\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import types\n",
    "import collections\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "#helper class to keep data\n",
    "class Node():\n",
    "    def __init__(self, column_index=None, question_split=None, left=None, right=None):\n",
    "\n",
    "        self.column_index = column_index\n",
    "        self.question_split = question_split\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "   \n",
    "\n",
    "class Leaf():\n",
    "    def __init__(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "#decision tree class\n",
    "class DecisionTree():\n",
    "    def __init__(self,allowed_depth=2):\n",
    "\n",
    "        #first node\n",
    "        self.root = None\n",
    "\n",
    "        #tree limit\n",
    "        self.allowed_depth = allowed_depth\n",
    "    \n",
    "    def build(self, X,Y, depth=0):\n",
    "\n",
    "        #number of rows and columns   \n",
    "        n_rows, n_Y = np.shape(X)\n",
    "        #check tree limit\n",
    "        if depth>=self.allowed_depth:\n",
    "            val = self.value(Y)\n",
    "            return(Leaf(val))\n",
    "        \n",
    "        if depth<self.allowed_depth:\n",
    "            #get split\n",
    "            column_index,right,left,qst = self.accurate_split(X,Y, n_rows, n_Y)\n",
    "            #if we have a split\n",
    "            if column_index is not None:\n",
    "                left_side = self.build(left[:, :-1],left[:, -1], depth+1)\n",
    "                right_side = self.build(right[:, :-1],right[:, -1], depth+1)\n",
    "                return Node(column_index, qst,left_side, right_side)\n",
    "            else:\n",
    "                val = self.value(Y)\n",
    "                return(Leaf(val))\n",
    " \n",
    "            \n",
    "    \n",
    "    def accurate_split(self, X,Y, n_x, n_y):\n",
    "    \n",
    "        column_index = None\n",
    "        left = None\n",
    "        right = None\n",
    "        question_split = None\n",
    "        total_ig = -99999999\n",
    "        \n",
    "        for column in range(n_y):\n",
    "            x_values = X[:, column]\n",
    "            \n",
    "            for question in x_values:\n",
    "                #calculate information gain\n",
    "                dataset = np.concatenate((X, Y.reshape(1, -1).T), axis=1)\n",
    "                data_left = np.array([row for row in dataset if row[column] <= question])\n",
    "                data_right = np.array([row for row in dataset if row[column] > question])\n",
    "                \n",
    "                if(len(data_left)>0 and len(data_right>0)):\n",
    "                    info_gain= self.calc_ig(dataset,data_left,data_right,column,question)\n",
    "                    #replace information gain\n",
    "                    if info_gain>total_ig and info_gain >0:\n",
    "                        column_index = column\n",
    "                        right = data_right\n",
    "                        left = data_left\n",
    "                        question_split = question\n",
    "                              \n",
    "        return column_index,right,left,question_split\n",
    "    \n",
    "\n",
    "    def calc_entropy(self,y,base = None):\n",
    "\n",
    "        q = np.bincount(np.array(y, dtype=np.int64))\n",
    "        ps = q / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])   \n",
    "\n",
    "    def calc_ig(self,dataset,dataset_left,dataset_right,col,qst):\n",
    "\n",
    "        data_left = dataset_left[:, -1]\n",
    "        data_right = dataset_right[:, -1]\n",
    "        data = dataset[:, -1]\n",
    "        #calculate left probability\n",
    "        left_weight = len(data_left) / len(data)\n",
    "        #calculate right probability\n",
    "        right_weight = len(data_right) / len(data)\n",
    "        #get the parent entropy\n",
    "        parent_entropy = self.calc_entropy(data)\n",
    "        #calculate entropy for the left and right side\n",
    "        left_child_entropy = left_weight*self.calc_entropy(data_left)\n",
    "        right_child_entropy = right_weight*self.calc_entropy(data_right)\n",
    "        #calculate gain\n",
    "        gain = parent_entropy- (left_child_entropy + right_child_entropy)\n",
    "        return gain\n",
    "\n",
    "    def find_leaves(self, X):\n",
    "\n",
    "        leaves = [self.find_leaf(x, self.root) for x in X]\n",
    "        return np.array(leaves)\n",
    "    \n",
    "    def find_leaf(self, x, node):\n",
    "        #return if leaf\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.value\n",
    "        feature_index = x[node.column_index]\n",
    "        \n",
    "        if feature_index<=node.question_split:\n",
    "            return self.find_leaf(x, node.left)\n",
    "        else:\n",
    "            return self.find_leaf(x, node.right)\n",
    "\n",
    " \n",
    "    #leaf node gets the most common dependent variable\n",
    "    def value(self, Y):\n",
    "        counter = Counter(Y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "    \n",
    "    #starter method\n",
    "    def fit(self, X,Y):\n",
    "        self.root = self.build(X,Y)\n",
    "    #calculate how many predictions were accurate\n",
    "    def acc(self,label,pred_label):\n",
    "        return np.sum(np.equal(label, pred_label)) / len(label)\n",
    "    #print number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m41\u001b[39m)\n\u001b[0;32m     11\u001b[0m decisiontree \u001b[38;5;241m=\u001b[39m  DecisionTree()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mdecisiontree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m decisiontree\u001b[38;5;241m.\u001b[39mfind_leaves(X_test)\n\u001b[0;32m     15\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "Cell \u001b[1;32mIn[146], line 9\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[146], line 16\u001b[0m, in \u001b[0;36mDecisionTree.build\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(y)\n\u001b[0;32m     15\u001b[0m column \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 16\u001b[0m threshold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[:, column]\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Ensure numpy array for threshold calculation\u001b[39;00m\n\u001b[0;32m     18\u001b[0m left_idx \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[:, column] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m     19\u001b[0m right_idx \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[:, column] \u001b[38;5;241m>\u001b[39m threshold\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "import decision_tree_prototype\n",
    "\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "diabetes = diabetes.drop(columns=['BloodPressure', 'SkinThickness'])\n",
    "\n",
    "X = diabetes.iloc[:,:-1].values\n",
    "y = diabetes.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "decisiontree =  DecisionTree()\n",
    "decisiontree.fit(X_train,y_train)\n",
    "y_pred = decisiontree.find_leaves(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, allowed_depth=3):\n",
    "        self.allowed_depth = allowed_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build(X, y, depth=0)\n",
    "\n",
    "    def build(self, X, y, depth):\n",
    "        if depth >= self.allowed_depth or len(np.unique(y)) == 1:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        column = np.random.randint(X.shape[1])\n",
    "        threshold = np.mean(X.iloc[:, column].values)  # Ensure numpy array for threshold calculation\n",
    "        \n",
    "        left_idx = X.iloc[:, column] <= threshold\n",
    "        right_idx = X.iloc[:, column] > threshold\n",
    "        \n",
    "        left = self.build(X[left_idx], y[left_idx], depth + 1)\n",
    "        right = self.build(X[right_idx], y[right_idx], depth + 1)\n",
    "        \n",
    "        return (column, threshold, left, right)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        def traverse(x, node):\n",
    "            if isinstance(node, float):  # Leaf node\n",
    "                return node\n",
    "            \n",
    "            column, threshold, left, right = node\n",
    "            \n",
    "            if x[column] <= threshold:\n",
    "                return traverse(x, left)\n",
    "            else:\n",
    "                return traverse(x, right)\n",
    "        \n",
    "        return np.array([traverse(x, self.root) for x in X.values])  # Ensure X is in numpy array form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df['Outcome']\n",
    "cols = ['Glucose', 'Pregnancies','DiabetesPedigreeFunction', 'Insulin', 'BMI', 'Age']\n",
    "Xdecision = df[cols]\n",
    "\n",
    "X_trainD, X_testD, y_trainD, y_testD = train_test_split(Xdecision, y, test_size=0.2, random_state=1234)\n",
    "X_trainD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Initialize and fit the DecisionTree\n",
    "deci = DecisionTree(allowed_depth=3)\n",
    "deci.fit(X_trainD, y_trainD)\n",
    "\n",
    "# Make predictions using the predict_proba method\n",
    "y_pred = deci.predict_proba(X_testD)\n",
    "\n",
    "# Since the decision tree outputs probabilities, threshold them to get class predictions\n",
    "y_pred_class = (y_pred > 0.5).astype(int)  # Assuming binary classification (threshold 0.5)\n",
    "\n",
    "# Calculate accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_testD, y_pred_class)\n",
    "conf_matrix = confusion_matrix(y_testD, y_pred_class)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deci =  DecisionTree()\n",
    "deci.fit(X_trainD,y_trainD)\n",
    "\n",
    "y_pred = deci.find_leaves(X_testD)\n",
    "\n",
    "accuracy = accuracy_score(y_testD, y_pred)\n",
    "conf_matrix = confusion_matrix(y_testD, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_prob_lr = log_reg.predict_proba(X_test)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
    "auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "\n",
    "\n",
    "\n",
    "knn = KNN(k=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_prob_knn = knn.predict_proba(X_test)\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_knn)\n",
    "auc_knn = roc_auc_score(y_test, y_prob_knn)\n",
    "\n",
    "\n",
    "decision_tree = DecisionTree(allowed_depth=3)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_prob_dt = decision_tree.predict_proba(X_test)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)\n",
    "auc_dt = roc_auc_score(y_test, y_prob_dt)\n",
    "\n",
    "# Plot ROC Curves for All Algorithms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.2f})')\n",
    "plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {auc_knn:.2f})')\n",
    "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
